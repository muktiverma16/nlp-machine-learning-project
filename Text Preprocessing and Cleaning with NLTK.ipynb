{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6aabe61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello  world!  this is a sample text with a url: https://example.com and some html <b>tags</b>.\n"
     ]
    }
   ],
   "source": [
    "#lowering text\n",
    "text = \"Hello  World!  This is a sample text with a URL: https://example.com and some HTML <b>tags</b>.\"\n",
    "text = text.lower()\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bf5a376e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'World', '!', 'This', 'is', 'a', 'sample', 'text', 'with', 'a', 'URL', ':', 'https', ':', '//example.com', 'and', 'some', 'HTML', '<', 'b', '>', 'tags', '<', '/b', '>', '.']\n"
     ]
    }
   ],
   "source": [
    "# Word Tokenization\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "text=\"Hello  World!  This is a sample text with a URL: https://example.com and some HTML <b>tags</b>.\"\n",
    "tokens=word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7dcae274-df97-49c0-b425-ae5f150d4bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'World', '!', 'This', 'is', 'a', 'sample', 'text', 'with', 'a', 'URL', ':', 'https', ':', '//example.com', 'and', 'some', 'HTML', '<', 'b', '>', 'tags', '<', '/b', '>', '.']\n",
      "[('Hello', 'NNP'), ('World', 'NNP'), ('!', '.'), ('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('sample', 'JJ'), ('text', 'NN'), ('with', 'IN'), ('a', 'DT'), ('URL', 'NN'), (':', ':'), ('https', 'NN'), (':', ':'), ('//example.com', 'NN'), ('and', 'CC'), ('some', 'DT'), ('HTML', 'NNP'), ('<', 'NNP'), ('b', 'NN'), ('>', 'NN'), ('tags', 'NNS'), ('<', 'VBP'), ('/b', 'JJ'), ('>', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# POS Tagging\n",
    "text=\"Hello  World!  This is a sample text with a URL: https://example.com and some HTML <b>tags</b>.\"\n",
    "import nltk\n",
    "#from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "#tokens=word_tokenize(text)\n",
    "print(tokens)\n",
    "print(pos_tag(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "356b0ead-2b9b-4289-8021-71345beee118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (GPE Apple/NNP)\n",
      "  targeted/VBN\n",
      "  by/IN\n",
      "  $/$\n",
      "  1/CD\n",
      "  billion/CD\n",
      "  class/NN\n",
      "  action/NN\n",
      "  lawsuit/NN\n",
      "  in/IN\n",
      "  UK/NNP)\n"
     ]
    }
   ],
   "source": [
    "# Named Entity Recognition (NER)\n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "sentence = \"Apple targeted by $1 billion class action lawsuit in UK\"\n",
    "chunks = ne_chunk(pos_tag(word_tokenize(sentence)))\n",
    "print(chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4ffbc27-3731-4a01-be4b-0b3fe1c307a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting beautifulsoup4Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Using cached beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4)\n",
      "  Using cached soupsieve-2.5-py3-none-any.whl.metadata (4.7 kB)\n",
      "Using cached beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "Using cached soupsieve-2.5-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4\n",
      "Successfully installed beautifulsoup4-4.12.3 soupsieve-2.5\n"
     ]
    }
   ],
   "source": [
    "pip install beautifulsoup4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc40617b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World! This is a sample text with a URL: https://example.com and some HTML tags.\n"
     ]
    }
   ],
   "source": [
    "# Remove HTML tags\n",
    "from bs4 import BeautifulSoup\n",
    "# Remove HTML tags\n",
    "text = \"Hello World! This is a sample text with a URL: https://example.com and some HTML <b>tags</b>.\"\n",
    "text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89afc2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World! This is a sample text with a URL:  and some HTML <b>tags</b>.\n"
     ]
    }
   ],
   "source": [
    "# Remove URLs\n",
    "import re\n",
    "text = \"Hello World! This is a sample text with a URL: https://example.com and some HTML <b>tags</b>.\"\n",
    "text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cefc3309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World! This is a sample text with a URL: https://example.com and some HTML <b>tags</b>.\n"
     ]
    }
   ],
   "source": [
    "# Remove unwanted white spaces\n",
    "text = \"Hello    World!    This is a sample text with a URL: https://example.com and some HTML <b>tags</b>.\"\n",
    "text = re.sub(r'\\s+', ' ', text).strip()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c16deb62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "Hello World This is a sample text with a URL https //example.com and some HTML b tags /b\n"
     ]
    }
   ],
   "source": [
    "#Removing punctuation\n",
    "\n",
    "import string\n",
    "\n",
    "# Print the punctuation characters\n",
    "print(string.punctuation)\n",
    "\n",
    "# Define the text\n",
    "text = \"Hello    World!    This is a sample text with a URL: https://example.com and some HTML <b>tags</b>.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Remove punctuation\n",
    "tokens = [word for word in tokens if word not in string.punctuation]\n",
    "\n",
    "# Join the tokens back into a string\n",
    "filtered_text = ' '.join(tokens)\n",
    "\n",
    "print(filtered_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9d9182b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World ! This sample text URL : https : //example.com HTML < b > tags < /b > . Barack Obama born Hawaii .\n"
     ]
    }
   ],
   "source": [
    "# Removing Stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "text = \"Hello World! This is a sample text with a URL: https://example.com and some HTML <b>tags</b>. Barack Obama was born in Hawaii.\"\n",
    "tokens = word_tokenize(text)\n",
    "filtered_words = [word for word in tokens if word not in stop_words]\n",
    "filtered_words = ' '.join(filtered_words)\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1de0af70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original words: ['Hello', 'World', '!', 'This', 'is', 'a', 'sample', 'text', 'with', 'some', 'words', '.']\n",
      "Stemmed words: ['hello', 'world', '!', 'thi', 'is', 'a', 'sampl', 'text', 'with', 'some', 'word', '.']\n"
     ]
    }
   ],
   "source": [
    "#stemming of the text\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "text = \"Hello World! This is a sample text with some words.\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Initialize the Porter Stemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# Apply stemming\n",
    "stemmed_words = [ps.stem(word) for word in words]\n",
    "\n",
    "print(\"Original words:\", words)\n",
    "print(\"Stemmed words:\", stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75e88eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "print(\"Lemmatized words:\", lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "906c85dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original words: ['Hello', 'World', '!', 'This', 'is', 'a', 'sample', 'text', 'with', 'some', 'words', '.']\n",
      "Lemmatized words: ['Hello', 'World', '!', 'This', 'is', 'a', 'sample', 'text', 'with', 'some', 'word', '.']\n"
     ]
    }
   ],
   "source": [
    "# Sample text\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "text = \"Hello World! This is a sample text with some words.\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Initialize the WordNet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Apply lemmatization\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "print(\"Original words:\", tokens)\n",
    "print(\"Lemmatized words:\", lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "954fd8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'World', '!', 'This', 'is', 'a', 'sample', 'text', 'with', 'some', 'words', '.']\n",
      "[('Hello', 'NNP'), ('World', 'NNP'), ('!', '.'), ('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('sample', 'JJ'), ('text', 'NN'), ('with', 'IN'), ('some', 'DT'), ('words', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# POS Tagging\n",
    "text=\"Hello  World!  This is a sample text with a URL: https://example.com and some HTML <b>tags</b>.\"\n",
    "import nltk\n",
    "#from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "#tokens=word_tokenize(text)\n",
    "print(tokens)\n",
    "print(pos_tag(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a5c0ae8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (GPE Apple/NNP)\n",
      "  targeted/VBN\n",
      "  by/IN\n",
      "  $/$\n",
      "  1/CD\n",
      "  billion/CD\n",
      "  class/NN\n",
      "  action/NN\n",
      "  lawsuit/NN\n",
      "  in/IN\n",
      "  UK/NNP)\n"
     ]
    }
   ],
   "source": [
    "# Named Entity Recognition (NER)\n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "sentence = \"Apple targeted by $1 billion class action lawsuit in UK\"\n",
    "chunks = ne_chunk(pos_tag(word_tokenize(sentence)))\n",
    "print(chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3e8734a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entities:\n",
      "(S\n",
      "  (PERSON Barack/NNP)\n",
      "  (PERSON Obama/NNP)\n",
      "  was/VBD\n",
      "  born/VBN\n",
      "  in/IN\n",
      "  (GPE Hawaii/NNP)\n",
      "  and/CC\n",
      "  became/VBD\n",
      "  the/DT\n",
      "  president/NN\n",
      "  of/IN\n",
      "  the/DT\n",
      "  (GPE United/NNP States/NNPS)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# Named Entity Recognition (NER)\n",
    "# Sample text\n",
    "text = \"Barack Obama was born in Hawaii and became the president of the United States.\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Perform POS tagging\n",
    "pos_tags = pos_tag(words)\n",
    "\n",
    "# Perform NER\n",
    "named_entities = ne_chunk(pos_tags)\n",
    "\n",
    "print(\"Named Entities:\")\n",
    "print(named_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fe5f4b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunked Text: (S\n",
      "  Barack/NNP\n",
      "  Obama/NNP\n",
      "  was/VBD\n",
      "  born/VBN\n",
      "  in/IN\n",
      "  Hawaii/NNP\n",
      "  and/CC\n",
      "  became/VBD\n",
      "  (NP the/DT president/NN)\n",
      "  of/IN\n",
      "  the/DT\n",
      "  United/NNP\n",
      "  States/NNPS\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# Chunking\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "chunked = cp.parse(pos_tags)\n",
    "print(\"Chunked Text:\", chunked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c7c9544c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunked Text:\n",
      "(S\n",
      "  (NP The/DT big/JJ black/JJ cat/NN)\n",
      "  is/VBZ\n",
      "  sleeping/VBG\n",
      "  on/IN\n",
      "  (NP the/DT mat/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "#chunking of a sentence\n",
    "\n",
    "# Sample sentence\n",
    "\n",
    "from nltk import pos_tag, RegexpParser\n",
    "sentence = \"The big black cat is sleeping on the mat.\"\n",
    "# Tokenize the sentence into words\n",
    "words = word_tokenize(sentence)\n",
    "\n",
    "# Perform POS tagging\n",
    "pos_tags = pos_tag(words)\n",
    "\n",
    "# Define chunking grammar\n",
    "grammar = r\"\"\"\n",
    "    NP: {<DT>?<JJ>*<NN>}    # Chunk sequences of DT, JJ, NN\n",
    "        {<NNP>+}            # Chunk consecutive proper nouns\n",
    "\"\"\"\n",
    "\n",
    "# Create a chunk parser\n",
    "cp = RegexpParser(grammar)\n",
    "\n",
    "# Parse the POS tagged words using the chunk parser\n",
    "chunked = cp.parse(pos_tags)\n",
    "\n",
    "print(\"Chunked Text:\")\n",
    "print(chunked)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
